{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1efe496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from dataset import DeepGlobeDataset\n",
    "from typing import TypedDict\n",
    "import segmentation_models_pytorch as smp\n",
    "from torchvision import transforms as T\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed50cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandartEvalConfig(TypedDict):\n",
    "    data_dir: str = \"../dataset/train\"\n",
    "    val_size: float = 0.2\n",
    "    test_size: float = 0.1\n",
    "    transform = None\n",
    "    target_transform = None\n",
    "    batch_size: int = 1\n",
    "    learning_rate: float = 2e-4\n",
    "    epochs: int = 5\n",
    "    encoder_name: str = \"resnet18\"\n",
    "    encoder_weights: str = \"imagenet\"\n",
    "    activation: str = \"logsoftmax\"\n",
    "    in_channels: int = 3\n",
    "    classes: int = 7\n",
    "    device: str = \"mps\"\n",
    "    checkpoints_dir: str = \"../src/checkpoints\"\n",
    "    freeze_encoder_layers: int = -2  # freeze encoder excluding 2 last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce9c3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "masks = []\n",
    "images.append(Image.open(\"../dataset/train/6399_sat.jpg\"))\n",
    "masks.append(Image.open(\"../dataset/train/6399_mask.png\"))\n",
    "images.append(Image.open(\"../dataset/train/10901_sat.jpg\"))\n",
    "masks.append(Image.open(\"../dataset/train/10901_mask.png\"))\n",
    "images.append(Image.open(\"../dataset/train/855_sat.jpg\"))\n",
    "masks.append(Image.open(\"../dataset/train/855_mask.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c307105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask2tensor(mask, resize=False):\n",
    "    mask_array = np.array(mask)\n",
    "    class_mask = np.zeros(mask_array.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    for rgb, label in DeepGlobeDataset.class_mapping.items():\n",
    "        class_mask[(mask_array == rgb).all(axis=-1)] = label\n",
    "\n",
    "    class_mask = torch.from_numpy(class_mask).long()\n",
    "    if resize:\n",
    "        class_mask = T.Resize((2464, 2464))(class_mask.unsqueeze(0)).squeeze(0)\n",
    "    return class_mask\n",
    "\n",
    "def img2tensor(image, resize=False):\n",
    "    image_tensor = T.ToTensor()(image)\n",
    "    if resize:\n",
    "        image_tensor = T.Resize((2464, 2464))(image_tensor)\n",
    "    return image_tensor\n",
    "\n",
    "dataset = []\n",
    "for img, mask in zip(images, masks):\n",
    "    img_tensor = img2tensor(img)\n",
    "    mask_tensor = mask2tensor(mask)\n",
    "    dataset.append((img_tensor, mask_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42ed470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = StandartEvalConfig\n",
    "model_path = f\"{cfg.checkpoints_dir}/pspnet_Epochs:{cfg.epochs}_lf:DiceLoss_lr:{cfg.learning_rate}_best.pth\"\n",
    "model = smp.PSPNet(\n",
    "    encoder_name=cfg.encoder_name,\n",
    "    encoder_weights=cfg.encoder_weights,\n",
    "    in_channels=cfg.in_channels,\n",
    "    classes=cfg.classes,\n",
    "    activation=cfg.activation,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(cfg.device)))\n",
    "\n",
    "pred_pspnet = []\n",
    "for img, mask in dataset:\n",
    "    model.to(cfg.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = img.unsqueeze(0).to(cfg.device)\n",
    "        x = model(x)\n",
    "        pred_pspnet.append(torch.argmax(x, dim=1).squeeze(0).cpu().numpy())\n",
    "del(model)\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fceac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = StandartEvalConfig\n",
    "model_path = f\"{cfg.checkpoints_dir}/unet_Epochs:{4}_lf:DiceLoss_lr:{cfg.learning_rate}_best.pth\"\n",
    "model = smp.Unet(\n",
    "    encoder_name=cfg.encoder_name,\n",
    "    encoder_weights=cfg.encoder_weights,\n",
    "    in_channels=cfg.in_channels,\n",
    "    classes=cfg.classes,\n",
    "    activation=cfg.activation,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(cfg.device)))\n",
    "\n",
    "pred_unet = []\n",
    "for img, mask in dataset:\n",
    "    model.to(cfg.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = img.unsqueeze(0).to(cfg.device)\n",
    "        x = model(x)\n",
    "        pred_unet.append(torch.argmax(x, dim=1).squeeze(0).cpu().numpy())\n",
    "del(model)\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c69f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fpn = []\n",
    "for img, mask in zip(images, masks):\n",
    "    img_tensor = img2tensor(img, resize=True)\n",
    "    mask_tensor = mask2tensor(mask, resize=True)\n",
    "    dataset_fpn.append((img_tensor, mask_tensor))\n",
    "cfg = StandartEvalConfig\n",
    "model_path = f\"{cfg.checkpoints_dir}/fpn_Epochs:{cfg.epochs}_lf:DiceLoss_lr:{cfg.learning_rate}_best.pth\"\n",
    "model = smp.FPN(\n",
    "    encoder_name=cfg.encoder_name,\n",
    "    encoder_weights=cfg.encoder_weights,\n",
    "    in_channels=cfg.in_channels,\n",
    "    classes=cfg.classes,\n",
    "    activation=cfg.activation,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device(cfg.device)))\n",
    "\n",
    "pred_fpn = []\n",
    "for img, mask in dataset_fpn:\n",
    "    model.to(cfg.device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = img.unsqueeze(0).to(cfg.device)\n",
    "        x = model(x)\n",
    "        pred_fpn.append(torch.argmax(x, dim=1).squeeze(0).cpu().numpy())\n",
    "gc.collect()\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b9c5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined(idx, save_path=None):\n",
    "    h = max(pred_unet[idx].shape[0], pred_fpn[idx].shape[0])\n",
    "    w = max(pred_unet[idx].shape[1], pred_fpn[idx].shape[1])\n",
    "    combined = Image.new(\"RGB\", (w * 3 + 10, h * 3 + 15))\n",
    "    preds = [pred_pspnet[idx], pred_unet[idx], pred_fpn[idx]]\n",
    "    for i, (pred) in enumerate(preds):\n",
    "        combined.paste(masks[idx].convert(\"RGB\"), (0, i * (h + 5)))\n",
    "        combined.paste(images[idx].convert(\"RGB\"), (w + 5, i * (h + 5)))\n",
    "        combined.paste(DeepGlobeDataset.label_to_rgb_mask(pred), (w * 2 + 10, i * (h + 5)))\n",
    "    if save_path:\n",
    "        combined.save(save_path)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11279c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined(0, save_path=\"../results/collected/comparison_0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eaf092",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined(1, save_path=\"../results/collected/comparison_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4648896",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_combined(2, save_path=\"../results/collected/comparison_2.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation-models-comparison",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
